{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahinuralam/notebooks/blob/main/Ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b46f99f4-ff98-474b-bf19-0428a7ee7d98",
      "metadata": {
        "id": "b46f99f4-ff98-474b-bf19-0428a7ee7d98"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "# Define the input shape and number of classes (Make sure to use the same values as in the training)\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "\n",
        "# Define the path to the Downloads folder\n",
        "train_data_dir = os.path.expanduser('~/Downloads/labeled_dataset_loco')\n",
        "\n",
        "# Create data generators for training and validation with augmentation\n",
        "train_data_gen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.4\n",
        ")\n",
        "\n",
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',  # Update class_mode to 'sparse'\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_data_gen.flow_from_directory(\n",
        "    train_data_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=batch_size,\n",
        "    class_mode='sparse',  # Update class_mode to 'sparse'\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Define the path to the labeled dataset directory\n",
        "test_data_dir = os.path.expanduser('~/Downloads/labeled_dataset_loco')  # Update this with the path to your labeled dataset\n",
        "\n",
        "# Create a data generator for testing\n",
        "test_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "test_generator = test_data_gen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=input_shape[:2],\n",
        "    batch_size=32,\n",
        "    class_mode='sparse',  # Use 'sparse' for numerical class labels\n",
        "    shuffle=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fa4298b-45a5-4857-803e-2f065df1c985",
      "metadata": {
        "id": "5fa4298b-45a5-4857-803e-2f065df1c985"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "import tensorflow as tf\n",
        "\n",
        "DenseNet121 = tf.keras.models.load_model('/home/mahin/Documents/notebook/Anomaly/DenseNet121.h5')\n",
        "DenseNet121 = Model(inputs=DenseNet121.inputs,\n",
        "                outputs=DenseNet121.outputs,\n",
        "                name='DenseNet121')\n",
        "\n",
        "EfficientNetB0 = load_model('/home/mahin/Documents/notebook/Anomaly/EfficientNetB0.h5')\n",
        "EfficientNetB0 = Model(inputs=EfficientNetB0.inputs,\n",
        "                outputs=EfficientNetB0.outputs,\n",
        "                name='EfficientNetB0')\n",
        "\n",
        "InceptionV3 = load_model('/home/mahin/Documents/notebook/Anomaly/InceptionV3.h5')\n",
        "InceptionV3 = Model(inputs=InceptionV3.inputs,\n",
        "                outputs=InceptionV3.outputs,\n",
        "                name='InceptionV3')\n",
        "\n",
        "ResNet50 = load_model('/home/mahin/Documents/notebook/Anomaly/ResNet50.h5')\n",
        "ResNet50 = Model(inputs=ResNet50.inputs,\n",
        "                outputs=ResNet50.outputs,\n",
        "                name='ResNet50')\n",
        "\n",
        "VGG16 = load_model('/home/mahin/Documents/notebook/Anomaly/VGG16.h5')\n",
        "VGG16 = Model(inputs=VGG16.inputs,\n",
        "                outputs=VGG16.outputs,\n",
        "                name='VGG16')\n",
        "\n",
        "Xception = load_model('/home/mahin/Documents/notebook/Anomaly/Xception.h5')\n",
        "Xception = Model(inputs=Xception.inputs,\n",
        "                outputs=Xception.outputs,\n",
        "                name='Xception')\n",
        "\n",
        "\n",
        "models = [DenseNet121, EfficientNetB0, InceptionV3, ResNet50, VGG16, Xception]\n",
        "\n",
        "model_input = Input(shape=(224, 224, 3))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = Average()(model_outputs)\n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
        "\n",
        "ensemble_model.compile(optimizer='adam',loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False),metrics=['accuracy'])\n",
        "\n",
        "history=ensemble_model.fit(\n",
        "  train_generator,\n",
        "  validation_data=val_generator,\n",
        "  epochs=25,\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53c60f21-0b42-49f7-a07c-73dd729098c9",
      "metadata": {
        "id": "53c60f21-0b42-49f7-a07c-73dd729098c9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.getcwd())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d63724-737c-4b3d-862d-508f1e4c9a67",
      "metadata": {
        "id": "69d63724-737c-4b3d-862d-508f1e4c9a67"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d1851e1-a717-4bc1-8f3e-052b95c18e89",
      "metadata": {
        "id": "1d1851e1-a717-4bc1-8f3e-052b95c18e89"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "from tensorflow.keras.applications import DenseNet121, EfficientNetB0, InceptionV3, ResNet50, VGG16, Xception\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths to the model weights\n",
        "subdir = '/home/mahin/Documents/notebook/Anomaly'\n",
        "dense_path = f'{subdir}/DensNet121.weights.h5'\n",
        "efficient_path = f'{subdir}/EfficientNetB0.h5'\n",
        "inception_path = f'{subdir}/InceptionV3.h5'\n",
        "resnet_path = f'{subdir}/ResNet50.h5'\n",
        "vgg_path = f'{subdir}/VGG16.h5'\n",
        "xception_path = f'{subdir}/Xception.h5'\n",
        "\n",
        "# Create and load models\n",
        "DenseNet121_model = DenseNet121(input_shape=(224, 224, 3), include_top=True)\n",
        "DenseNet121_model.load_weights(dense_path)\n",
        "\n",
        "EfficientNetB0_model = EfficientNetB0(input_shape=(224, 224, 3), include_top=True)\n",
        "EfficientNetB0_model.load_weights(efficient_path)\n",
        "\n",
        "InceptionV3_model = InceptionV3(input_shape=(224, 224, 3), include_top=True)\n",
        "InceptionV3_model.load_weights(inception_path)\n",
        "\n",
        "ResNet50_model = ResNet50(input_shape=(224, 224, 3), include_top=True)\n",
        "ResNet50_model.load_weights(resnet_path)\n",
        "\n",
        "VGG16_model = VGG16(input_shape=(224, 224, 3), include_top=True)\n",
        "VGG16_model.load_weights(vgg_path)\n",
        "\n",
        "Xception_model = Xception(input_shape=(224, 224, 3), include_top=True)\n",
        "Xception_model.load_weights(xception_path)\n",
        "\n",
        "models = [DenseNet121_model, EfficientNetB0_model, InceptionV3_model, ResNet50_model, VGG16_model, Xception_model]\n",
        "\n",
        "model_input = Input(shape=(224, 224, 3))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = Average()(model_outputs)\n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
        "\n",
        "ensemble_model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "history = ensemble_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=25,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01ef5a0a-7f86-4843-b9db-c8696fbd24a0",
      "metadata": {
        "id": "01ef5a0a-7f86-4843-b9db-c8696fbd24a0"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.layers import Input, Average\n",
        "import tensorflow as tf\n",
        "\n",
        "# Define paths to the full model files\n",
        "subdir = '/home/mahin/Documents/notebook/Anomaly'\n",
        "dense_path = f'{subdir}/DenseNet121.h5'\n",
        "efficient_path = f'{subdir}/EfficientNetB0.h5'\n",
        "inception_path = f'{subdir}/InceptionV3.h5'\n",
        "resnet_path = f'{subdir}/ResNet50.h5'\n",
        "vgg_path = f'{subdir}/VGG16.h5'\n",
        "xception_path = f'{subdir}/Xception.h5'\n",
        "\n",
        "# Load the full models\n",
        "DenseNet121_model = load_model(dense_path)\n",
        "EfficientNetB0_model = load_model(efficient_path)\n",
        "InceptionV3_model = load_model(inception_path)\n",
        "ResNet50_model = load_model(resnet_path)\n",
        "VGG16_model = load_model(vgg_path)\n",
        "Xception_model = load_model(xception_path)\n",
        "\n",
        "models = [DenseNet121_model, EfficientNetB0_model, InceptionV3_model, ResNet50_model, VGG16_model, Xception_model]\n",
        "\n",
        "model_input = Input(shape=(224, 224, 3))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = Average()(model_outputs)\n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
        "\n",
        "ensemble_model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=False), metrics=['accuracy'])\n",
        "\n",
        "history = ensemble_model.fit(\n",
        "    train_generator,\n",
        "    validation_data=val_generator,\n",
        "    epochs=25,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5137d707-8b39-4e08-9769-3009a0b81cb2",
      "metadata": {
        "id": "5137d707-8b39-4e08-9769-3009a0b81cb2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "model1 = load_model('DenseNet121.h5')\n",
        "\n",
        "model2 = load_model('InceptionV3.h5')\n",
        "\n",
        "models = [model1, model2]\n",
        "\n",
        "def ensemble_predictions(members, testX):\n",
        " yhats = [model.predict(testX) for model in members]\n",
        " yhats = np.array(yhats)\n",
        " # sum across ensemble members\n",
        " summed = np.sum(yhats, axis=0)\n",
        " # argmax across classes\n",
        " result = np.argmax(summed, axis=1)\n",
        " return result\n",
        "\n",
        "\n",
        "# example of predicting\n",
        "img = plt.imread(test_dir + file)\n",
        "\n",
        "# Resize it to the net input size:\n",
        "img = cv2.resize(img, (224, 224))\n",
        "img = img[np.newaxis, ...]\n",
        "\n",
        "# Convert the data to float:\n",
        "img = img.astype(np.float32)\n",
        "\n",
        "class_index = ensemble_predictions(models, img)[0]\n",
        "\n",
        "\n",
        "# Convert class id to name\n",
        "\n",
        "\n",
        "label = class_names[class_index]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af6a7cd1-14a0-46de-b759-4cb2dfe7fed2",
      "metadata": {
        "id": "af6a7cd1-14a0-46de-b759-4cb2dfe7fed2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the Xception model\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "ResNet50 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "ResNet50.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "# Create checkpoint callback\n",
        "checkpoint_path = 'checkpoint/ResNet50.weights.h5'\n",
        "checkpoint_callback = ModelCheckpoint(checkpoint_path,\n",
        "                                      save_weights_only=True,\n",
        "                                      monitor=\"val_accuracy\",\n",
        "                                      save_best_only=True)\n",
        "\n",
        "# Setup EarlyStopping callback to stop training if model's val_loss doesn't improve for 3 epochs\n",
        "early_stopping = EarlyStopping(monitor = \"val_loss\", # watch the val loss metric\n",
        "                               patience = 5,\n",
        "                               restore_best_weights = True) # if val loss decreases for 3 epochs in a row, stop training\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "history1 = ResNet50.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f77cb8-91e5-4cb1-baf5-8638f7010727",
      "metadata": {
        "id": "68f77cb8-91e5-4cb1-baf5-8638f7010727"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the VGG16 model\n",
        "base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "VGG16 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "VGG16.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# Train the model\n",
        "history2 = VGG16.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b3630e-0d79-416c-a05a-2e51fa75cdca",
      "metadata": {
        "id": "09b3630e-0d79-416c-a05a-2e51fa75cdca"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.xception import Xception\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the VGG16 model\n",
        "base_model = Xception(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "Xception = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "Xception.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# Train the model\n",
        "history3 = Xception.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c7f943a-6740-4130-a0a4-8c6031407272",
      "metadata": {
        "id": "7c7f943a-6740-4130-a0a4-8c6031407272"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the VGG16 model\n",
        "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "InceptionV3 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "InceptionV3.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# Train the model\n",
        "history4 = InceptionV3.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8154b830-cfb4-4a8b-b823-0714c035ea98",
      "metadata": {
        "id": "8154b830-cfb4-4a8b-b823-0714c035ea98"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the VGG16 model\n",
        "base_model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "EfficientNetB0 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "EfficientNetB0.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# Train the model\n",
        "history4 = EfficientNetB0.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd9d9bc9-03ca-4022-807f-eeefc981e00e",
      "metadata": {
        "id": "fd9d9bc9-03ca-4022-807f-eeefc981e00e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.applications.densenet import DenseNet121\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dropout\n",
        "\n",
        "\n",
        "\n",
        "# Define the input shape and number of classes\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 5  # Update this line with the correct number of classes in your dataset\n",
        "\n",
        "# Create the VGG16 model\n",
        "base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "x = Dropout(0.2)(x)\n",
        "predictions = Dense(num_classes, activation='softmax')(x)  # Change activation to 'softmax'\n",
        "DenseNet121 = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Adjust the batch size, learning rate, and augmentation parameters\n",
        "batch_size = 32\n",
        "learning_rate = 0.0001\n",
        "\n",
        "# Compile the model\n",
        "DenseNet121.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "# model.summary()\n",
        "\n",
        "# Train the model\n",
        "history6 = DenseNet121.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=35\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6237c9b-b257-481c-9f11-5546b487c3cf",
      "metadata": {
        "id": "d6237c9b-b257-481c-9f11-5546b487c3cf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Average\n",
        "\n",
        "\n",
        "models = [ResNet50, VGG16, Xception, InceptionV3, EfficientNetB0, DenseNet121]\n",
        "\n",
        "model_input = Input(shape=(224, 224, 3))\n",
        "model_outputs = [model(model_input) for model in models]\n",
        "ensemble_output = Average()(model_outputs)\n",
        "ensemble_model = Model(inputs=model_input, outputs=ensemble_output, name='ensemble')\n",
        "\n",
        "ensemble_model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n",
        "\n",
        "history = ensemble_model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=60,\n",
        "        callbacks=[\n",
        "        early_stopping,\n",
        "        checkpoint_callback,\n",
        "        reduce_lr\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f948d8-50ff-4dbd-baa0-5ad1d4431211",
      "metadata": {
        "id": "a2f948d8-50ff-4dbd-baa0-5ad1d4431211"
      },
      "outputs": [],
      "source": [
        "# Plot the training and validation accuracy\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot the training and validation loss using a logarithmic scale\n",
        "plt.plot(range(1, len(train_loss) + 1), np.log(train_loss), label='Training Loss')\n",
        "plt.plot(range(1, len(val_loss) + 1), np.log(val_loss), label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Log Loss')\n",
        "plt.title('Training and Validation Loss (Log Scale)')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the accuracy plot\n",
        "plt.figure()\n",
        "plt.plot(range(1, len(train_accuracy) + 1), train_accuracy, label='Training Accuracy')\n",
        "plt.plot(range(1, len(val_accuracy) + 1), val_accuracy, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "# Save the accuracy plot as an image\n",
        "plt.savefig('accuracy_plot.png', bbox_inches='tight')\n",
        "\n",
        "# Calculate and display the best validation accuracy and loss\n",
        "best_val_accuracy = max(val_accuracy)\n",
        "best_val_loss = min(val_loss)\n",
        "print(f'Best Validation Accuracy: {best_val_accuracy:.4f}')\n",
        "print(f'Best Validation Loss: {best_val_loss:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16d608e-cec1-4ea3-a12f-8d6e27d9f8e4",
      "metadata": {
        "id": "b16d608e-cec1-4ea3-a12f-8d6e27d9f8e4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Make predictions on the test data\n",
        "true_labels = test_generator.classes\n",
        "predictions = ensemble_model.predict(test_generator)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Calculate the anomaly percentage\n",
        "total_images = len(true_labels)\n",
        "anomaly_count = np.sum(predicted_labels != true_labels)\n",
        "anomaly_percentage = (anomaly_count / total_images) * 100\n",
        "\n",
        "# Print the anomaly percentage and accuracy\n",
        "accuracy = (1 - (anomaly_count / total_images)) * 100\n",
        "print(f'Accuracy: {accuracy:.2f}%')\n",
        "print(f'Anomaly Percentage: {anomaly_percentage:.2f}%')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54e1def9-cc67-4d30-afba-c2e7171fa35b",
      "metadata": {
        "id": "54e1def9-cc67-4d30-afba-c2e7171fa35b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# true_labels are the actual labels from your test data\n",
        "# predicted_labels are the labels predicted by your model\n",
        "\n",
        "# Generate the confusion matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels, normalize='true')\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(10, 10))  # Adjust the size as needed\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_generator.class_indices.keys())\n",
        "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
        "plt.xticks(rotation=45)  # Rotate labels to avoid overlap\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}